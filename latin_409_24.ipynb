{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b53dcf6d-b26b-4921-b30b-3782b5940cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, getopt, re\n",
    "from functools import wraps\n",
    "from glob import glob\n",
    "import pickle\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1657a8-4820-4ded-9d34-362e04c83ea3",
   "metadata": {},
   "source": [
    "# Latin Data Splits and Verb Suffixing Corrections\n",
    "This project was broken up into two main parts: Creating the data splits, and editing the baseline code to more accurately predict the morphological inflections of the 什么什么什么"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc661865-c712-4d28-835c-4ee9cd5d5b7f",
   "metadata": {},
   "source": [
    "## A. Splitting the Data\n",
    "Since Latin had no splits, our group needed to manually make the train, test, and dev sets. This task alone has a lot of factors to consider to avoid artificially inflating the accuracy of our code. Additionally, we started with nearly one million lines of data, which needed to be cut down\n",
    "\n",
    "There are many factors to keep in mind while splitting the data. Below are the two main factors we focused on:\n",
    "\n",
    "#### Size of the data\n",
    "\n",
    "The size of the training data correlates with the accuracy of the code (Kodner et al. 2023). By cutting down the data, we save both on artificially raised accuracy and runtime\n",
    "\n",
    "#### Splitting-by-form vs. Splitting-by-lemma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4602276-0152-4f1c-a035-b7e9886c03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming(s,t):\n",
    "    return sum(1 for x,y in zip(s,t) if x != y)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c6cbf-89e2-4d2d-aa4f-58c110bc085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def halign(s,t):\n",
    "    \"\"\"Align two strings by Hamming distance.\"\"\"\n",
    "    slen = len(s)\n",
    "    tlen = len(t)\n",
    "    minscore = len(s) + len(t) + 1\n",
    "    for upad in range(0, len(t)+1):\n",
    "        upper = '_' * upad + s + (len(t) - upad) * '_'\n",
    "        lower = len(s) * '_' + t\n",
    "        score = hamming(upper, lower)\n",
    "        if score < minscore:\n",
    "            bu = upper\n",
    "            bl = lower\n",
    "            minscore = score\n",
    "\n",
    "    for lpad in range(0, len(s)+1):\n",
    "        upper = len(t) * '_' + s\n",
    "        lower = (len(s) - lpad) * '_' + t + '_' * lpad\n",
    "        score = hamming(upper, lower)\n",
    "        if score < minscore:\n",
    "            bu = upper\n",
    "            bl = lower\n",
    "            minscore = score\n",
    "\n",
    "    zipped = list(zip(bu,bl))\n",
    "    newin  = ''.join(i for i,o in zipped if i != '_' or o != '_')\n",
    "    newout = ''.join(o for i,o in zipped if i != '_' or o != '_')\n",
    "    return newin, newout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58c6f165-a221-4af7-9f5a-8d4a81170a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s, t, inscost = 1.0, delcost = 1.0, substcost = 1.0):\n",
    "    \"\"\"Recursive implementation of Levenshtein, with alignments returned.\"\"\"\n",
    "    @memolrec\n",
    "    def lrec(spast, tpast, srem, trem, cost):\n",
    "        if len(srem) == 0:\n",
    "            return spast + len(trem) * '_', tpast + trem, '', '', cost + len(trem)\n",
    "        if len(trem) == 0:\n",
    "            return spast + srem, tpast + len(srem) * '_', '', '', cost + len(srem)\n",
    "\n",
    "        addcost = 0\n",
    "        if srem[0] != trem[0]:\n",
    "            addcost = substcost\n",
    "\n",
    "        return min((lrec(spast + srem[0], tpast + trem[0], srem[1:], trem[1:], cost + addcost),\n",
    "                   lrec(spast + '_', tpast + trem[0], srem, trem[1:], cost + inscost),\n",
    "                   lrec(spast + srem[0], tpast + '_', srem[1:], trem, cost + delcost)),\n",
    "                   key = lambda x: x[4])\n",
    "\n",
    "    answer = lrec('', '', s, t, 0)\n",
    "    return answer[0],answer[1],answer[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6382cc70-b997-4c73-ae58-b7561145c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memolrec(func):\n",
    "    \"\"\"Memoizer for Levenshtein.\"\"\"\n",
    "    cache = {}\n",
    "    @wraps(func)\n",
    "    def wrap(sp, tp, sr, tr, cost):\n",
    "        if (sr,tr) not in cache:\n",
    "            res = func(sp, tp, sr, tr, cost)\n",
    "            cache[(sr,tr)] = (res[0][len(sp):], res[1][len(tp):], res[4] - cost)\n",
    "        return sp + cache[(sr,tr)][0], tp + cache[(sr,tr)][1], '', '', cost + cache[(sr,tr)][2]\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ac44438-98a5-4bf3-9ba8-2a42e3aa2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignprs(lemma, form):\n",
    "    \"\"\"Break lemma/form into three parts:\n",
    "    IN:  1 | 2 | 3\n",
    "    OUT: 4 | 5 | 6\n",
    "    1/4 are assumed to be prefixes, 2/5 the stem, and 3/6 a suffix.\n",
    "    1/4 and 3/6 may be empty.\n",
    "    \"\"\"\n",
    "\n",
    "    al = levenshtein(lemma, form, substcost = 1.1) # Force preference of 0:x or x:0 by 1.1 cost\n",
    "    alemma, aform = al[0], al[1]\n",
    "    # leading spaces\n",
    "    lspace = max(len(alemma) - len(alemma.lstrip('_')), len(aform) - len(aform.lstrip('_')))\n",
    "    # trailing spaces\n",
    "    tspace = max(len(alemma[::-1]) - len(alemma[::-1].lstrip('_')), len(aform[::-1]) - len(aform[::-1].lstrip('_')))\n",
    "    return alemma[0:lspace], alemma[lspace:len(alemma)-tspace], alemma[len(alemma)-tspace:], aform[0:lspace], aform[lspace:len(alemma)-tspace], aform[len(alemma)-tspace:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb568175-949d-4431-8d9d-5f6df5a8cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_suffix_rules_get(lemma, form):\n",
    "    \"\"\"Extract a number of suffix-change and prefix-change rules\n",
    "    based on a given example lemma+inflected form.\"\"\"\n",
    "    lp,lr,ls,fp,fr,fs = alignprs(lemma, form) # Get six parts, three for in three for out\n",
    "\n",
    "    # Suffix rules\n",
    "    ins  = lr + ls + \">\"\n",
    "    outs = fr + fs + \">\"\n",
    "    srules = set()\n",
    "    for i in range(min(len(ins), len(outs))):\n",
    "        srules.add((ins[i:], outs[i:]))\n",
    "    srules = {(x[0].replace('_',''), x[1].replace('_','')) for x in srules}\n",
    "\n",
    "    # Prefix rules\n",
    "    prules = set()\n",
    "    if len(lp) >= 0 or len(fp) >= 0:\n",
    "        inp = \"<\" + lp\n",
    "        outp = \"<\" + fp\n",
    "        for i in range(0,len(fr)):\n",
    "            prules.add((inp + fr[:i],outp + fr[:i]))\n",
    "            prules = {(x[0].replace('_',''), x[1].replace('_','')) for x in prules}\n",
    "\n",
    "    return prules, srules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df571781-324c-4afc-8fa1-05446680c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_best_rule(lemma, msd, allprules, allsrules):\n",
    "    \"\"\"Applies the longest-matching suffix-changing rule given an input\n",
    "    form and the MSD. Length ties in suffix rules are broken by frequency.\n",
    "    For prefix-changing rules, only the most frequent rule is chosen.\"\"\"\n",
    "\n",
    "    bestrulelen = 0\n",
    "    base = \"<\" + lemma + \">\"\n",
    "    if msd not in allprules and msd not in allsrules:\n",
    "        return lemma # Haven't seen this inflection, so bail out\n",
    "\n",
    "    if msd in allsrules:\n",
    "        applicablerules = [(x[0],x[1],y) for x,y in allsrules[msd].items() if x[0] in base]\n",
    "        if applicablerules:\n",
    "            bestrule = max(applicablerules, key = lambda x: (len(x[0]), x[2], len(x[1])))\n",
    "            base = base.replace(bestrule[0], bestrule[1])\n",
    "\n",
    "    if msd in allprules:\n",
    "        applicablerules = [(x[0],x[1],y) for x,y in allprules[msd].items() if x[0] in base]\n",
    "        if applicablerules:\n",
    "            bestrule = max(applicablerules, key = lambda x: (x[2]))\n",
    "            base = base.replace(bestrule[0], bestrule[1])\n",
    "\n",
    "    base = base.replace('<', '')\n",
    "    base = base.replace('>', '')\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d38c11f-30b3-4bb5-8553-590b6a66ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numleadingsyms(s, symbol):\n",
    "    return len(s) - len(s.lstrip(symbol))\n",
    "\n",
    "\n",
    "def numtrailingsyms(s, symbol):\n",
    "    return len(s) - len(s.rstrip(symbol))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f016884-2cd4-466a-b156-fb2c4af728f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73885ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input file as an array so I can skip over entries with +s\n",
    "lines = []\n",
    "with open('Latin_stuff/ORIGINAL_lat.trn') as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "    \n",
    "# Read in data as dataframe\n",
    "lat = pd.read_table(\"Latin_stuff/ORIGINAL_lat.trn\", sep='\\t', names=['Lemon', 'Infection', 'Infected'], skiprows=lambda x: '+' in lines[x])\n",
    "print(lat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917da537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the unique lemmas and infinitives\n",
    "infinitives = lat[lat['Infection'] == 'V;NFIN;ACT;PRS']\n",
    "uniqueLemmas = lat.drop_duplicates(subset = ['Lemon', 'PartoSpeech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3848fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the unique lemmas into dataframes by part of speech while also cutting it down using numbers I calculated elsewhere\n",
    "partSample = uniqueLemmas[uniqueLemmas['PartoSpeech'] == 'V.PTCP'].sample(n=112)\n",
    "adjSample = uniqueLemmas[uniqueLemmas['PartoSpeech'] == 'ADJ'].sample(n=105)\n",
    "nounSample = uniqueLemmas[uniqueLemmas['PartoSpeech'] == 'N'].sample(n=185)\n",
    "verbSample = uniqueLemmas[uniqueLemmas['PartoSpeech'] == 'V'].sample(n=41)\n",
    "propSample = uniqueLemmas[uniqueLemmas['PartoSpeech'] == 'PROPN'].sample(n=343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframes randomly into train, test, and dev sets in a 10:1:1 ratio\n",
    "partTrain, partTest = train_test_split(partSample, test_size=2000/12000, random_state=56)\n",
    "partTest, partDev = train_test_split(partTest, test_size=0.5, random_state=56)\n",
    "\n",
    "adjTrain, adjTest = train_test_split(adjSample, test_size=2000/12000, random_state=56)\n",
    "adjTest, adjDev = train_test_split(adjTest, test_size=0.5, random_state=56)\n",
    "\n",
    "nounTrain, nounTest = train_test_split(nounSample, test_size=2000/12000, random_state=56)\n",
    "nounTest, nounDev = train_test_split(nounTest, test_size=0.5, random_state=56)\n",
    "\n",
    "verbTrain, verbTest = train_test_split(verbSample, test_size=2000/12000, random_state=56)\n",
    "verbTest, verbDev = train_test_split(verbTest, test_size=0.5, random_state=56)\n",
    "\n",
    "propTrain, propTest = train_test_split(propSample, test_size=2000/12000, random_state=56)\n",
    "propTest, propDev = train_test_split(propTest, test_size=0.5, random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28555a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes together to get all the unique lemmas in the test, train, and dev sets\n",
    "uniqueTrain = pd.concat([partTrain, adjTrain, nounTrain, verbTrain, propTrain])\n",
    "uniqueTest = pd.concat([partTest, adjTest, nounTest, verbTest, propTest])\n",
    "uniqueDev = pd.concat([partDev, adjDev, nounDev, verbDev, propDev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4823c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get all of the other lemmas which match with the lemmas in the unique list and return it as a dataframe\n",
    "def getlist(lemons):\n",
    "    splitslist = []\n",
    "    for lemon in lemons['Lemon']:\n",
    "        for row in lat[lat['Lemon'] == lemon].to_numpy().tolist():\n",
    "            splitslist.append(row)\n",
    "    return pd.DataFrame(splitslist, columns= [\"Lemon\", \"Infected\", \"Infection\", \"PartoSpeech\"])\n",
    "\n",
    "# Call the method to get the full train, test, and dev sets\n",
    "train = getlist(uniqueTrain)\n",
    "test = getlist(uniqueTest)\n",
    "dev = getlist(uniqueDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34925a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the sizes of the train, test, and dev sets to check that they look right\n",
    "print(\"Train set size\", train.shape)\n",
    "print(\"Test set size\", test.shape)\n",
    "print(\"Dev set size\", dev.shape)\n",
    "\n",
    "# Print the number of rows per part of speech in the training set to check that they look right\n",
    "print(\"Train set split into parts of speech\")\n",
    "print(train[train['PartoSpeech'] == 'V.PTCP'].shape)\n",
    "print(train[train['PartoSpeech'] == 'ADJ'].shape)\n",
    "print(train[train['PartoSpeech'] == 'N'].shape)\n",
    "print(train[train['PartoSpeech'] == 'V'].shape)\n",
    "print(train[train['PartoSpeech'] == 'PROPN'].shape)\n",
    "\n",
    "# Print the number of rows per part of speech in the test set to check that they look right\n",
    "print(\"Test set split into parts of speech\")\n",
    "print(test[test['PartoSpeech'] == 'V.PTCP'].shape)\n",
    "print(test[test['PartoSpeech'] == 'ADJ'].shape)\n",
    "print(test[test['PartoSpeech'] == 'N'].shape)\n",
    "print(test[test['PartoSpeech'] == 'V'].shape)\n",
    "print(test[test['PartoSpeech'] == 'PROPN'].shape)\n",
    "\n",
    "# Print the number of rows per part of speech in the dev set to check that they look right\n",
    "print(\"Dev set split into parts of speech\")\n",
    "print(dev[dev['PartoSpeech'] == 'V.PTCP'].shape)\n",
    "print(dev[dev['PartoSpeech'] == 'ADJ'].shape)\n",
    "print(dev[dev['PartoSpeech'] == 'N'].shape)\n",
    "print(dev[dev['PartoSpeech'] == 'V'].shape)\n",
    "print(dev[dev['PartoSpeech'] == 'PROPN'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2632473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a method to convert the dataframes to files based on Emily's code\n",
    "def toFile(frame, fileName, fileType):\n",
    "    frame.to_csv(path_or_buf= './Latin_stuff/' + fileName + fileType,sep= \"\\t\", encoding= \"utf8\", index= False, header=False, columns= [\"Lemon\", \"Infected\", \"Infection\"])\n",
    "\n",
    "# Convert the test, train, and dev sets to files\n",
    "toFile(train, 'lat', '.trn')\n",
    "toFile(test, 'lat', '.tst')\n",
    "toFile(dev, 'lat', '.dev')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
