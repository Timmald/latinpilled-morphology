{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53dcf6d-b26b-4921-b30b-3782b5940cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, getopt, re\n",
    "from functools import wraps\n",
    "from glob import glob\n",
    "import pickle\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1657a8-4820-4ded-9d34-362e04c83ea3",
   "metadata": {},
   "source": [
    "# Latin Data Splits and Verb Suffixing Corrections\n",
    "This project was broken up into two main parts: Creating the data splits, and editing the baseline code to more accurately predict the morphological inflections of latin words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc661865-c712-4d28-835c-4ee9cd5d5b7f",
   "metadata": {},
   "source": [
    "## A. Splitting the Data\n",
    "Since Latin had no splits, our group needed to manually make the train, test, and dev sets. This task alone has a lot of factors to consider to avoid artificially inflating the accuracy of our code. Additionally, we started with nearly one million lines of data, which needed to be cut down\n",
    "\n",
    "There are many factors to keep in mind while splitting the data. Below are the two main factors we focused on:\n",
    "\n",
    "#### Size of the data\n",
    "\n",
    "The size of the training data correlates with the accuracy of the code (Kodner et al. 2023). By cutting down the data, we save both on artificially raised accuracy and runtime\n",
    "\n",
    "#### Splitting-by-form vs. Splitting-by-lemma\n",
    "\n",
    "Splitting-by-form takes random lines from the data to add to either the test, train, or dev sets. This can let the code \"cheat\" if it is trained on a lemma that also shows up in another form in the test or dev set. In contrast, if you split-by-lemma (where you take every inflection for a single lemma and put it in one of the splits) you can avoid overlap. This gives us a better idea of the accuracy of the code in a more applicable situation where the code only sees the lemma and the msd.(Goldman et al. 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4602276-0152-4f1c-a035-b7e9886c03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming(s,t):\n",
    "    return sum(1 for x,y in zip(s,t) if x != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28c6cbf-89e2-4d2d-aa4f-58c110bc085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def halign(s,t):\n",
    "    \"\"\"Align two strings by Hamming distance.\"\"\"\n",
    "    slen = len(s)\n",
    "    tlen = len(t)\n",
    "    minscore = len(s) + len(t) + 1\n",
    "    for upad in range(0, len(t)+1):\n",
    "        upper = '_' * upad + s + (len(t) - upad) * '_'\n",
    "        lower = len(s) * '_' + t\n",
    "        score = hamming(upper, lower)\n",
    "        if score < minscore:\n",
    "            bu = upper\n",
    "            bl = lower\n",
    "            minscore = score\n",
    "\n",
    "    for lpad in range(0, len(s)+1):\n",
    "        upper = len(t) * '_' + s\n",
    "        lower = (len(s) - lpad) * '_' + t + '_' * lpad\n",
    "        score = hamming(upper, lower)\n",
    "        if score < minscore:\n",
    "            bu = upper\n",
    "            bl = lower\n",
    "            minscore = score\n",
    "\n",
    "    zipped = list(zip(bu,bl))\n",
    "    newin  = ''.join(i for i,o in zipped if i != '_' or o != '_')\n",
    "    newout = ''.join(o for i,o in zipped if i != '_' or o != '_')\n",
    "    return newin, newout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c6f165-a221-4af7-9f5a-8d4a81170a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s, t, inscost = 1.0, delcost = 1.0, substcost = 1.0):\n",
    "    \"\"\"Recursive implementation of Levenshtein, with alignments returned.\"\"\"\n",
    "    @memolrec\n",
    "    def lrec(spast, tpast, srem, trem, cost):\n",
    "        if len(srem) == 0:\n",
    "            return spast + len(trem) * '_', tpast + trem, '', '', cost + len(trem)\n",
    "        if len(trem) == 0:\n",
    "            return spast + srem, tpast + len(srem) * '_', '', '', cost + len(srem)\n",
    "\n",
    "        addcost = 0\n",
    "        if srem[0] != trem[0]:\n",
    "            addcost = substcost\n",
    "\n",
    "        return min((lrec(spast + srem[0], tpast + trem[0], srem[1:], trem[1:], cost + addcost),\n",
    "                   lrec(spast + '_', tpast + trem[0], srem, trem[1:], cost + inscost),\n",
    "                   lrec(spast + srem[0], tpast + '_', srem[1:], trem, cost + delcost)),\n",
    "                   key = lambda x: x[4])\n",
    "\n",
    "    answer = lrec('', '', s, t, 0)\n",
    "    return answer[0],answer[1],answer[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6382cc70-b997-4c73-ae58-b7561145c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memolrec(func):\n",
    "    \"\"\"Memoizer for Levenshtein.\"\"\"\n",
    "    cache = {}\n",
    "    @wraps(func)\n",
    "    def wrap(sp, tp, sr, tr, cost):\n",
    "        if (sr,tr) not in cache:\n",
    "            res = func(sp, tp, sr, tr, cost)\n",
    "            cache[(sr,tr)] = (res[0][len(sp):], res[1][len(tp):], res[4] - cost)\n",
    "        return sp + cache[(sr,tr)][0], tp + cache[(sr,tr)][1], '', '', cost + cache[(sr,tr)][2]\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac44438-98a5-4bf3-9ba8-2a42e3aa2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignprs(lemma, form):\n",
    "    \"\"\"Break lemma/form into three parts:\n",
    "    IN:  1 | 2 | 3\n",
    "    OUT: 4 | 5 | 6\n",
    "    1/4 are assumed to be prefixes, 2/5 the stem, and 3/6 a suffix.\n",
    "    1/4 and 3/6 may be empty.\n",
    "    \"\"\"\n",
    "\n",
    "    al = levenshtein(lemma, form, substcost = 1.1) # Force preference of 0:x or x:0 by 1.1 cost\n",
    "    alemma, aform = al[0], al[1]\n",
    "    # leading spaces\n",
    "    lspace = max(len(alemma) - len(alemma.lstrip('_')), len(aform) - len(aform.lstrip('_')))\n",
    "    # trailing spaces\n",
    "    tspace = max(len(alemma[::-1]) - len(alemma[::-1].lstrip('_')), len(aform[::-1]) - len(aform[::-1].lstrip('_')))\n",
    "    return alemma[0:lspace], alemma[lspace:len(alemma)-tspace], alemma[len(alemma)-tspace:], aform[0:lspace], aform[lspace:len(alemma)-tspace], aform[len(alemma)-tspace:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb568175-949d-4431-8d9d-5f6df5a8cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_suffix_rules_get(lemma, form):\n",
    "    \"\"\"Extract a number of suffix-change and prefix-change rules\n",
    "    based on a given example lemma+inflected form.\"\"\"\n",
    "    lp,lr,ls,fp,fr,fs = alignprs(lemma, form) # Get six parts, three for in three for out\n",
    "\n",
    "    # Suffix rules\n",
    "    ins  = lr + ls + \">\"\n",
    "    outs = fr + fs + \">\"\n",
    "    srules = set()\n",
    "    for i in range(min(len(ins), len(outs))):\n",
    "        srules.add((ins[i:], outs[i:]))\n",
    "    srules = {(x[0].replace('_',''), x[1].replace('_','')) for x in srules}\n",
    "\n",
    "    # Prefix rules\n",
    "    prules = set()\n",
    "    if len(lp) >= 0 or len(fp) >= 0:\n",
    "        inp = \"<\" + lp\n",
    "        outp = \"<\" + fp\n",
    "        for i in range(0,len(fr)):\n",
    "            prules.add((inp + fr[:i],outp + fr[:i]))\n",
    "            prules = {(x[0].replace('_',''), x[1].replace('_','')) for x in prules}\n",
    "\n",
    "    return prules, srules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df571781-324c-4afc-8fa1-05446680c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_best_rule(lemma, msd, allprules, allsrules):\n",
    "    \"\"\"Applies the longest-matching suffix-changing rule given an input\n",
    "    form and the MSD. Length ties in suffix rules are broken by frequency.\n",
    "    For prefix-changing rules, only the most frequent rule is chosen.\"\"\"\n",
    "\n",
    "    bestrulelen = 0\n",
    "    base = \"<\" + lemma + \">\"\n",
    "    if msd not in allprules and msd not in allsrules:\n",
    "        return lemma # Haven't seen this inflection, so bail out\n",
    "\n",
    "    if msd in allsrules:\n",
    "        applicablerules = [(x[0],x[1],y) for x,y in allsrules[msd].items() if x[0] in base]\n",
    "        if applicablerules:\n",
    "            bestrule = max(applicablerules, key = lambda x: (len(x[0]), x[2], len(x[1])))\n",
    "            base = base.replace(bestrule[0], bestrule[1])\n",
    "\n",
    "    if msd in allprules:\n",
    "        applicablerules = [(x[0],x[1],y) for x,y in allprules[msd].items() if x[0] in base]\n",
    "        if applicablerules:\n",
    "            bestrule = max(applicablerules, key = lambda x: (x[2]))\n",
    "            base = base.replace(bestrule[0], bestrule[1])\n",
    "\n",
    "    base = base.replace('<', '')\n",
    "    base = base.replace('>', '')\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d38c11f-30b3-4bb5-8553-590b6a66ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numleadingsyms(s, symbol):\n",
    "    return len(s) - len(s.lstrip(symbol))\n",
    "\n",
    "\n",
    "def numtrailingsyms(s, symbol):\n",
    "    return len(s) - len(s.rstrip(symbol))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84095685",
   "metadata": {},
   "source": [
    "### A1. Creating splits by Lemma using pandas.DataFrame\n",
    "At first Emily was having issues remembering the column names, so we named the Lemma, Inflection, and Inflected columns Lemon, Infection, and Infected respectively.  We also added a 4th column, PartoSpeech, to make splitting the lemmas by part of speech easier.\n",
    "\n",
    "We started by getting rid of all the rows with duplicate lemmas. Then, we sampled a number of each part of speech(Noun, Proper Nouns, Verb, Participle, Adjective) such that when you get the inflected for each lemma, there will be a similar number of inflected for each part of speech.\n",
    "We also took this as chance to replace the original lemmas of verbs, which were the 1st principle parts of verb, with the present active Infinitive, which provides more information regarding the conjugation of verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73885ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(815989, 3)\n"
     ]
    }
   ],
   "source": [
    "# Get the input file as an array so I can skip over entries with +s\n",
    "latPath = os.path.join('..', 'Latin_stuff', 'lat.txt')\n",
    "lines = []\n",
    "with open(latPath) as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "    \n",
    "# Read in data as dataframe\n",
    "# Note: Lemon = lemmas, Infection = inflections, and Infected = inflected\n",
    "lat = pd.read_table(latPath, sep='\\t', names=['Lemon', 'Infection', 'Infected'], skiprows=lambda x: '+' in lines[x])\n",
    "print(lat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "917da537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the present active infinitives so that they can be used as the verb lemmas\n",
    "lat['PartoSpeech'] = lat['Infection'].str.extract(r'(N|PROPN|V|V.PTCP|ADJ);')\n",
    "infinitives = lat[lat['Infection'] == 'V;NFIN;ACT;PRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd778e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemon</th>\n",
       "      <th>Infection</th>\n",
       "      <th>Infected</th>\n",
       "      <th>PartoSpeech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213333</th>\n",
       "      <td>imitƒÅrƒ´</td>\n",
       "      <td>V;IND;ACT;PRS;1;SG</td>\n",
       "      <td>imitor</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213334</th>\n",
       "      <td>imitƒÅrƒ´</td>\n",
       "      <td>V;IND;ACT;PRS;2;SG</td>\n",
       "      <td>imitƒÅris</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213335</th>\n",
       "      <td>imitƒÅrƒ´</td>\n",
       "      <td>V;IND;ACT;PRS;2;SG</td>\n",
       "      <td>imitƒÅre</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213336</th>\n",
       "      <td>imitƒÅrƒ´</td>\n",
       "      <td>V;IND;ACT;PRS;3;SG</td>\n",
       "      <td>imitƒÅtur</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213337</th>\n",
       "      <td>imitƒÅrƒ´</td>\n",
       "      <td>V;IND;ACT;PRS;1;PL</td>\n",
       "      <td>imitƒÅmur</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Lemon           Infection  Infected PartoSpeech\n",
       "213333  imitƒÅrƒ´  V;IND;ACT;PRS;1;SG    imitor           V\n",
       "213334  imitƒÅrƒ´  V;IND;ACT;PRS;2;SG  imitƒÅris           V\n",
       "213335  imitƒÅrƒ´  V;IND;ACT;PRS;2;SG   imitƒÅre           V\n",
       "213336  imitƒÅrƒ´  V;IND;ACT;PRS;3;SG  imitƒÅtur           V\n",
       "213337  imitƒÅrƒ´  V;IND;ACT;PRS;1;PL  imitƒÅmur           V"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def switch_verb_lemma(lemma):\n",
    "    inf = infinitives[infinitives[\"Lemon\"]==lemma][\"Infected\"].iloc[0]\n",
    "    return inf\n",
    "\n",
    "lat.loc[lat[\"Infection\"].str.startswith(\"V;\"),\"Lemon\"]=lat[lat[\"Infection\"].str.startswith(\"V;\")][\"Lemon\"].map(switch_verb_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3848fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Lemon           Infection Infected PartoSpeech\n",
      "213333    imitƒÅrƒ´  V;IND;ACT;PRS;1;SG   imitor           V\n",
      "213387       cƒ´re  V;IND;ACT;PRS;1;SG      ci≈ç           V\n",
      "213472  aurƒìscere  V;IND;ACT;PRS;1;SG  aurƒìsc≈ç           V\n",
      "213514     senƒìre  V;IND;ACT;PRS;1;SG    sene≈ç           V\n",
      "213598      vƒìscƒ´  V;IND;ACT;PRS;1;SG   vƒìscor           V\n",
      "(41, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a list of unique lemmas to use for the splits, to avoid the same lemma in training and test\n",
    "uniqueLemmas = lat.drop_duplicates(subset = ['Lemon', 'PartoSpeech'])\n",
    "# Split the unique lemmas into dataframes by part of speech while also cutting it down using numbers I calculated elsewhere\n",
    "# This variable is to make sure the data gets split/sampled the same way everytime, making it a variable for ease\n",
    "rand = 34\n",
    "partSample = uniqueLemmas[uniqueLemmas['PartoSpeech'] == 'V.PTCP'].sample(n=112, random_state=rand)\n",
    "adjSample = uniqueLemmas[uniqueLemmas['PartoSpeech'] == 'ADJ'].sample(n=105, random_state=rand)\n",
    "nounSample = uniqueLemmas[uniqueLemmas['PartoSpeech'] == 'N'].sample(n=185, random_state=rand)\n",
    "verbSample = uniqueLemmas[uniqueLemmas['PartoSpeech'] == 'V'].sample(n=41, random_state=rand)\n",
    "propSample = uniqueLemmas[uniqueLemmas['PartoSpeech'] == 'PROPN'].sample(n=343, random_state=rand)\n",
    "print(verbSample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84e8f557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 4) (3, 4) (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataframes randomly into train, test, and dev sets in a 10:1:1 ratio\n",
    "partTrain, partTest = train_test_split(partSample, test_size=2000/12000, random_state=rand)\n",
    "partTest, partDev = train_test_split(partTest, test_size=0.5, random_state=rand)\n",
    "\n",
    "adjTrain, adjTest = train_test_split(adjSample, test_size=2000/12000, random_state=rand)\n",
    "adjTest, adjDev = train_test_split(adjTest, test_size=0.5, random_state=rand)\n",
    "\n",
    "nounTrain, nounTest = train_test_split(nounSample, test_size=2000/12000, random_state=rand)\n",
    "nounTest, nounDev = train_test_split(nounTest, test_size=0.5, random_state=rand)\n",
    "\n",
    "verbTrain, verbTest = train_test_split(verbSample, test_size=2000/12000, random_state=rand)\n",
    "verbTest, verbDev = train_test_split(verbTest, test_size=0.5, random_state=rand)\n",
    "\n",
    "propTrain, propTest = train_test_split(propSample, test_size=2000/12000, random_state=rand)\n",
    "propTest, propDev = train_test_split(propTest, test_size=0.5, random_state=rand)\n",
    "\n",
    "print(verbTrain.shape, verbTest.shape, verbDev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28555a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes together to get all the unique lemmas in the test, train, and dev sets\n",
    "uniqueTrain = pd.concat([partTrain, adjTrain, nounTrain, verbTrain, propTrain])\n",
    "uniqueTest = pd.concat([partTest, adjTest, nounTest, verbTest, propTest])\n",
    "uniqueDev = pd.concat([partDev, adjDev, nounDev, verbDev, propDev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4823c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get all of the other lemmas which match with the lemmas in the unique list and return it as a dataframe\n",
    "def getlist(lemons):\n",
    "    splitslist = []\n",
    "    for lemon in lemons['Lemon']:\n",
    "        for row in lat[lat['Lemon'] == lemon].to_numpy().tolist():\n",
    "            splitslist.append(row)\n",
    "    return pd.DataFrame(splitslist, columns= [\"Lemon\", \"Infection\", \"Infected\", \"PartoSpeech\"])\n",
    "\n",
    "# Call the method to get the full train, test, and dev sets\n",
    "train = getlist(uniqueTrain)\n",
    "test = getlist(uniqueTest)\n",
    "dev = getlist(uniqueDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34925a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size (9893, 4)\n",
      "Test set size (943, 4)\n",
      "Dev set size (1138, 4)\n",
      "Train set split into parts of speech\n",
      "(2236, 4)\n",
      "(1955, 4)\n",
      "(1926, 4)\n",
      "(1819, 4)\n",
      "(1957, 4)\n",
      "Test set split into parts of speech\n",
      "(180, 4)\n",
      "(184, 4)\n",
      "(182, 4)\n",
      "(180, 4)\n",
      "(217, 4)\n",
      "Dev set split into parts of speech\n",
      "(208, 4)\n",
      "(262, 4)\n",
      "(229, 4)\n",
      "(233, 4)\n",
      "(206, 4)\n"
     ]
    }
   ],
   "source": [
    "# Print the sizes of the train, test, and dev sets to check that they look right\n",
    "print(\"Train set size\", train.shape)\n",
    "print(\"Test set size\", test.shape)\n",
    "print(\"Dev set size\", dev.shape)\n",
    "\n",
    "# Print the number of rows per part of speech in the training set to check that they look right\n",
    "print(\"Train set split into parts of speech\")\n",
    "print(train[train['PartoSpeech'] == 'V.PTCP'].shape)\n",
    "print(train[train['PartoSpeech'] == 'ADJ'].shape)\n",
    "print(train[train['PartoSpeech'] == 'N'].shape)\n",
    "print(train[train['PartoSpeech'] == 'V'].shape)\n",
    "print(train[train['PartoSpeech'] == 'PROPN'].shape)\n",
    "\n",
    "# Print the number of rows per part of speech in the test set to check that they look right\n",
    "print(\"Test set split into parts of speech\")\n",
    "print(test[test['PartoSpeech'] == 'V.PTCP'].shape)\n",
    "print(test[test['PartoSpeech'] == 'ADJ'].shape)\n",
    "print(test[test['PartoSpeech'] == 'N'].shape)\n",
    "print(test[test['PartoSpeech'] == 'V'].shape)\n",
    "print(test[test['PartoSpeech'] == 'PROPN'].shape)\n",
    "\n",
    "# Print the number of rows per part of speech in the dev set to check that they look right\n",
    "print(\"Dev set split into parts of speech\")\n",
    "print(dev[dev['PartoSpeech'] == 'V.PTCP'].shape)\n",
    "print(dev[dev['PartoSpeech'] == 'ADJ'].shape)\n",
    "print(dev[dev['PartoSpeech'] == 'N'].shape)\n",
    "print(dev[dev['PartoSpeech'] == 'V'].shape)\n",
    "print(dev[dev['PartoSpeech'] == 'PROPN'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2632473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a method to convert the dataframes to files based on Emily's code\n",
    "def toFile(frame, fileName, fileType):\n",
    "    frame.to_csv(path_or_buf= '../Latin_stuff/' + fileName + fileType,sep= \"\\t\", encoding= \"utf8\", index= False, header=False, columns= [\"Lemon\", \"Infection\", \"Infected\"])\n",
    "\n",
    "# Convert the test, train, and dev sets to files\n",
    "toFile(train, 'lat', '.trn')\n",
    "toFile(test, 'lat', '.tst')\n",
    "toFile(dev, 'lat', '.dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ad1af",
   "metadata": {},
   "source": [
    "## NOW WE CAN TRAIN OUR THINGY! WOOOOO! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Editing nonneural\n",
    "When we first got the nonneural code, it wasn't very efficient and very time consuming to run(runtime of O(n^3)), so we used pickle to store data, so after the code runs for the first time, all subsequent runs of the same splits are a lot faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b343097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9893/9893 [00:04<00:00, 2030.45it/s]\n"
     ]
    }
   ],
   "source": [
    "#Finds rules from training data\n",
    "prefbias,suffbias = 0,1\n",
    "allprules, allsrules = {}, {}\n",
    "trnPath = os.path.join('..', 'Latin_stuff', 'lat.trn')\n",
    "lines = [line.strip() for line in open(trnPath, \"r\", encoding='utf8') if line != '\\n']\n",
    "for l in tqdm.tqdm(lines): # Read in lines and extract transformation rules from pairs\n",
    "            lemma, msd, form = l.split(u'\\t')\n",
    "            if prefbias > suffbias:\n",
    "                lemma = lemma[::-1]\n",
    "                form = form[::-1]\n",
    "            prules, srules = prefix_suffix_rules_get(lemma, form)\n",
    "\n",
    "            if msd not in allprules and len(prules) > 0:\n",
    "                allprules[msd] = {}\n",
    "            if msd not in allsrules and len(srules) > 0:\n",
    "                allsrules[msd] = {}\n",
    "\n",
    "            for r in prules:\n",
    "                if (r[0],r[1]) in allprules[msd]:\n",
    "                    allprules[msd][(r[0],r[1])] = allprules[msd][(r[0],r[1])] + 1\n",
    "                else:\n",
    "                    allprules[msd][(r[0],r[1])] = 1\n",
    "\n",
    "            for r in srules:\n",
    "                if (r[0],r[1]) in allsrules[msd]:\n",
    "                    allsrules[msd][(r[0],r[1])] = allsrules[msd][(r[0],r[1])] + 1\n",
    "                else:\n",
    "                    allsrules[msd][(r[0],r[1])] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92aabd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1138/1138 [00:00<00:00, 6455.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9112478031634447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#now we apply the rules we found to the dev set!\n",
    "devPath = os.path.join('..', 'Latin_stuff', 'lat.dev')\n",
    "devlines = [line.strip() for line in open(devPath, \"r\", encoding='utf8') if line != '\\n']\n",
    "numcorrect = 0\n",
    "numguesses = 0\n",
    "for l in tqdm.tqdm(devlines):\n",
    "    lemma, msd, correct = l.split(u'\\t')\n",
    "#                    lemma, msd, = l.split(u'\\t')\n",
    "    if prefbias > suffbias:\n",
    "        lemma = lemma[::-1]\n",
    "    outform = apply_best_rule(lemma, msd, allprules, allsrules)\n",
    "    if prefbias > suffbias:\n",
    "        outform = outform[::-1]\n",
    "        lemma = lemma[::-1]\n",
    "    if outform == correct:\n",
    "        numcorrect += 1\n",
    "    numguesses += 1\n",
    "print(f\"acc: {numcorrect/numguesses}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe23394",
   "metadata": {},
   "source": [
    "# DAMN. (‚äôÀç‚äô)\n",
    "## C. Results\n",
    "When we originally ran the splits without changing the lemmas, we got an average accuracy of 83%. \n",
    "\n",
    "As you can see, with all the changes, we reached a average accuracy of 91%, which I think is pretty good score ¬Ø\\ _ („ÉÑ) _/¬Ø\n",
    "\n",
    "·ìö·òè·ó¢\n",
    "### „Éæ(Ôø£‚ñΩÔø£)Bye~Bye~\n",
    "\n",
    "(footnote from Nathan finishing this whilst eepy): (‚Ä¢_‚Ä¢) (¬∞_¬∞)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b1626",
   "metadata": {},
   "source": [
    "# Reading Citations:\n",
    "\n",
    "Omer Goldman, David Guriel, and Reut Tsarfaty. 2022. (Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models‚Äô Performance. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 864‚Äì870, Dublin, Ireland. Association for Computational Linguistics.\n",
    "\n",
    "Jordan Kodner, Sarah Payne, Salam Khalifa, and Zoey Liu. 2023. Morphological Inflection: A Reality Check. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6082‚Äì6101, Toronto, Canada. Association for Computational Linguistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "409",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
